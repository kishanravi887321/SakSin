"""
LangChain Client Implementations
Professional client classes for different AI providers
"""

import logging
from typing import Dict, Any, List, Optional, AsyncGenerator
from abc import ABC, abstractmethod

try:
    from google import genai
    GOOGLE_GENAI_AVAILABLE = True
    from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
except ImportError as e:
    print(f"âš ï¸  Google GenAI package not installed: {e}")
    print("ğŸ’¡ Run: pip install google-genai")
    GOOGLE_GENAI_AVAILABLE = False
    
    # Fallback to old LangChain if available
    try:
        from langchain_google_genai import ChatGoogleGenerativeAI
        from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        print("ğŸ”„ Falling back to langchain-google-genai")
        GOOGLE_GENAI_AVAILABLE = False
    except ImportError:
        print("âŒ Neither google-genai nor langchain-google-genai available")
        raise ImportError("Please install: pip install google-genai")

from .config import LangChainConfig
from .utils import ErrorHandler, ResponseFormatter

logger = logging.getLogger(__name__)


class BaseLLMClient(ABC):
    """Abstract base class for LLM clients"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.error_handler = ErrorHandler()
        self.formatter = ResponseFormatter()
    
    @abstractmethod
    def generate_response(self, prompt: str, **kwargs) -> str:
        """Generate a response from the LLM"""
        pass
    
    @abstractmethod
    async def generate_response_async(self, prompt: str, **kwargs) -> str:
        """Generate a response from the LLM asynchronously"""
        pass


class GeminiClient(BaseLLMClient):
    """Professional Google Gemini client supporting both new SDK and fallback"""
    
    def __init__(self):
        config = LangChainConfig.get_gemini_config()
        super().__init__(config)
        
        try:
            if GOOGLE_GENAI_AVAILABLE:
                # Configure the new Google GenAI SDK
                # Set the API key as environment variable for automatic pickup
                import os
                os.environ['GEMINI_API_KEY'] = config['api_key']
                
                # Initialize the new client (no api_key parameter needed)
                self.client = genai.Client()
                self.model = config['model']
                self.use_new_sdk = True
                logger.info(f"âœ… Gemini client initialized with new Google GenAI SDK, using model: {self.model}")
            else:
                # Fallback to old LangChain integration
                self.llm = ChatGoogleGenerativeAI(
                    model=config['model'],
                    temperature=config['temperature'],
                    max_tokens=config['max_tokens'],
                    top_p=config['top_p'],
                    top_k=config['top_k'],
                    google_api_key=config['api_key'],
                    verbose=LangChainConfig.VERBOSE
                )
                self.output_parser = StrOutputParser()
                self.use_new_sdk = False
                logger.info("âœ… Gemini client initialized with LangChain fallback")
            
            self.temperature = config['temperature']
            self.max_tokens = config['max_tokens']
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize Gemini client: {e}")
            raise
    
    def list_available_models(self) -> List[str]:
        """List available Gemini models"""
        try:
            if self.use_new_sdk:
                models_response = self.client.models.list()
                model_names = [model.name for model in models_response]
                logger.info(f"ğŸ“‹ Available models: {model_names}")
                return model_names
            else:
                logger.info("âš ï¸ Model listing only available with new SDK")
                return []
        except Exception as e:
            logger.error(f"âŒ Error listing models: {e}")
            return []
    
    def generate_response(self, prompt: str, system_message: Optional[str] = None, **kwargs) -> str:
        """
        Generate a response using Gemini (supports both new SDK and fallback)
        
        Args:
            prompt: User prompt
            system_message: Optional system message for context
            **kwargs: Additional parameters
            
        Returns:
            Generated response string
        """
        try:
            if self.use_new_sdk:
                # Use new Google GenAI SDK with model fallback
                full_prompt = prompt
                if system_message:
                    full_prompt = f"System: {system_message}\n\nUser: {prompt}"
                
                # Try primary model first, then fallbacks
                models_to_try = [self.model] + LangChainConfig.get_fallback_models()
                models_to_try = list(dict.fromkeys(models_to_try))  # Remove duplicates
                
                response_text = None
                last_error = None
                
                for model_name in models_to_try:
                    try:
                        logger.info(f"ğŸ”„ Trying model: {model_name}")
                        response = self.client.models.generate_content(
                            model=model_name,
                            contents=full_prompt
                        )
                        response_text = response.text
                        logger.info(f"âœ… Success with model: {model_name}")
                        
                        # Update the working model for future use
                        if model_name != self.model:
                            self.model = model_name
                            logger.info(f"ğŸ”„ Updated working model to: {model_name}")
                        
                        break
                    except Exception as e:
                        last_error = e
                        if "404" in str(e) or "not found" in str(e).lower():
                            logger.warning(f"âš ï¸ Model {model_name} not available: {e}")
                            continue
                        else:
                            logger.error(f"âŒ Error with model {model_name}: {e}")
                            raise e
                
                if response_text is None:
                    raise last_error or Exception("All models failed")
                    
            else:
                # Use LangChain fallback
                messages = []
                if system_message:
                    messages.append(SystemMessage(content=system_message))
                messages.append(HumanMessage(content=prompt))
                
                chain = self.llm | self.output_parser
                response_text = chain.invoke(messages)
            
            print(f"Prompt: {prompt}")
            logger.info(f"âœ… Generated response for prompt: {prompt[:50]}...")
            return self.formatter.format_response(response_text)
            
        except Exception as e:
            logger.error(f"âŒ Error generating response: {e}")
            return self.error_handler.handle_generation_error(e)
    
    async def generate_response_async(self, prompt: str, system_message: Optional[str] = None, **kwargs) -> str:
        """
        Generate a response using Gemini asynchronously (supports both SDKs)
        
        Args:
            prompt: User prompt
            system_message: Optional system message for context
            **kwargs: Additional parameters
            
        Returns:
            Generated response string
        """
        try:
            if self.use_new_sdk:
                # Use new Google GenAI SDK (sync for now)
                full_prompt = prompt
                if system_message:
                    full_prompt = f"System: {system_message}\n\nUser: {prompt}"
                
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=full_prompt
                )
                
                response_text = response.text
            else:
                # Use LangChain fallback with async
                messages = []
                if system_message:
                    messages.append(SystemMessage(content=system_message))
                messages.append(HumanMessage(content=prompt))
                
                chain = self.llm | self.output_parser
                response_text = await chain.ainvoke(messages)
            
            logger.info(f"âœ… Generated async response for prompt: {prompt[:50]}...")
            return self.formatter.format_response(response_text)
            
        except Exception as e:
            logger.error(f"âŒ Error generating async response: {e}")
            return self.error_handler.handle_generation_error(e)
    
    def generate_interview_response(self, question: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate interview-specific response with structured output
        
        Args:
            question: Interview question
            context: Interview context (role, experience, etc.)
            
        Returns:
            Structured interview response
        """
        try:
            system_message = f"""
            You are an AI interview assistant for SÄká¹£in, a professional interview platform.
            
            Context:
            - Role: {context.get('role', 'General')}
            - Experience Level: {context.get('experience', 'Entry Level')}
            - Industry: {context.get('industry', 'Technology')}
            
            Provide a professional, constructive response that:
            1. Evaluates the answer quality
            2. Provides specific feedback
            3. Suggests improvements
            4. Gives a score (1-10)
            
            Be encouraging but honest in your assessment.
            """
            
            prompt = f"Interview Question: {question}\n\nPlease provide your analysis and feedback."
            
            response = self.generate_response(prompt, system_message)
            
            # Parse structured response
            return self.formatter.format_interview_response(response, context)
            
        except Exception as e:
            logger.error(f"âŒ Error generating interview response: {e}")
            return self.error_handler.handle_interview_error(e)
    
    def generate_chat_response(self, message: str, conversation_history: List[Dict] = None) -> str:
        """
        Generate chat response with conversation context (supports both SDKs)
        
        Args:
            message: Current user message
            conversation_history: Previous conversation messages
            
        Returns:
            Chat response string
        """
        try:
            # Add system message for chat context
            system_message = """
            You are a helpful AI assistant for SÄká¹£in, a professional interview platform.
            Provide helpful, professional responses to user queries about interviews,
            career advice, and platform usage. Be concise but informative
            
ğŸ›¡ï¸ Guidelines:
- Stay neutral and professional.
- Avoid overly generic adviceâ€”tailor responses to the role and context.
- If uncertain about a domain, ask clarifying questions or suggest exploration paths.

ğŸ¯ Example Behavior:
- For a frontend developer: Ask about React concepts, UI accessibility, or CSS performance.
- For a data role: Focus on SQL, data modeling, Python for data analysis, or ML pipeline questions.
- For a general role: Stick to HR questions, behavioral STAR-based answers, and confidence-building strategies.
            """
            
            if self.use_new_sdk:
                # Use new Google GenAI SDK
                context_parts = [f"System: {system_message}"]
                
                # Add conversation history
                if conversation_history:
                    for msg in conversation_history[-5:]:  # Last 5 messages
                        if msg['role'] == 'user':
                            context_parts.append(f"User: {msg['content']}")
                        elif msg['role'] == 'assistant':
                            context_parts.append(f"Assistant: {msg['content']}")
                
                # Add current message
                context_parts.append(f"User: {message}")
                
                full_context = "\n\n".join(context_parts)
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=full_context
                )
                
                response_text = response.text
            else:
                # Use LangChain fallback
                messages = [SystemMessage(content=system_message)]
                
                # Add conversation history
                if conversation_history:
                    for msg in conversation_history[-5:]:  # Last 5 messages
                        if msg['role'] == 'user':
                            messages.append(HumanMessage(content=msg['content']))
                        elif msg['role'] == 'assistant':
                            messages.append(AIMessage(content=msg['content']))
                
                # Add current message
                messages.append(HumanMessage(content=message))
                
                chain = self.llm | self.output_parser
                response_text = chain.invoke(messages)
            
            logger.info(f"âœ… Generated chat response for: {message[:50]}...")
            return self.formatter.format_chat_response(response_text)
            
        except Exception as e:
            logger.error(f"âŒ Error generating chat response: {e}")
            return self.error_handler.handle_chat_error(e)
    
    def generate_analysis(self, data: str, analysis_type: str = "general") -> Dict[str, Any]:
        """
        Generate analysis for various data types
        
        Args:
            data: Data to analyze
            analysis_type: Type of analysis (sentiment, performance, etc.)
            
        Returns:
            Analysis results
        """
        try:
            analysis_prompts = {
                "sentiment": "Analyze the sentiment of the following text and provide a detailed breakdown:",
                "performance": "Analyze the performance data and provide insights and recommendations:",
                "interview": "Analyze this interview response and provide detailed feedback:",
                "general": "Provide a comprehensive analysis of the following data:"
            }
            
            prompt = f"{analysis_prompts.get(analysis_type, analysis_prompts['general'])}\n\n{data}"
            
            system_message = """
            You are an expert data analyst. Provide structured, actionable insights
            with clear recommendations and supporting evidence.
            """
            
            response = self.generate_response(prompt, system_message)
            
            return self.formatter.format_analysis_response(response, analysis_type)
            
        except Exception as e:
            logger.error(f"âŒ Error generating analysis: {e}")
            return self.error_handler.handle_analysis_error(e)


# Client factory for easy instantiation
class ClientFactory:
    """Factory class for creating LLM clients"""
    
    @staticmethod
    def create_gemini_client() -> GeminiClient:
        """Create and return a Gemini client"""
        return GeminiClient()
    
    @staticmethod
    def get_default_client() -> GeminiClient:
        """Get the default LLM client (Gemini)"""
        return ClientFactory.create_gemini_client()